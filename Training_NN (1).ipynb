{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHjQ0X_WtAbm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Set path to the folder where your data is stored\n",
        "######################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = False)\n",
        "os.chdir('/content/drive/MyDrive/Neural Nets Spring 2025 Project')\n",
        "# os.chdir('/content/sample_data')\n",
        "os.getcwd()\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "9zB_oYL7pX-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Folds of Data for Training-Validation"
      ],
      "metadata": {
        "id": "m0e08XjGLaaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Read in Cached Datasets and Features\n",
        "cleandata = pd.read_feather('Data/cleandata.feather')\n",
        "gameavgstats = pd.read_feather('Data/gameavgstats.feather')\n",
        "sumstats = pd.read_feather('Data/sumstats.feather')\n",
        "teamstats = pd.read_feather('Data/teamstats.feather')\n",
        "otherstats = pd.read_feather('Data/otherstats.feather')"
      ],
      "metadata": {
        "id": "-j69sTACRz2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Function for returning training and validation data tensors for a fold (window) of the dataset based on seasons\n",
        "####Can specify number of seasons to use for validation and how many days after the training start date to not include (buffer)\n",
        "####Combines all features from and measures coverage for each player-game and removes those below a provided threshold\n",
        "####Imputes missing feature data by forward filling and FT features with 0s\n",
        "####Standardizes numeric features and returns training and validation tensors\n",
        "\n",
        "feature_list = []\n",
        "\n",
        "def get_data_fold(fold_start, fold_end, validation_years = 1, train_start_buffer = 60,  feature_cover_cutoff = 0.6, feature_list = None, testset = False):\n",
        "\n",
        "  validation_split = fold_end - validation_years + 1\n",
        "\n",
        "  print(\"STARTING DATA PREP FOR FOLD\")\n",
        "  #Get data for seasons in fold\n",
        "  alldf = cleandata.loc[(cleandata.Season >= fold_start) & (cleandata.Season <= fold_end), ['PlayerId','Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS']]\n",
        "\n",
        "  first_game_date = alldf.Date.min()\n",
        "  last_game_date = alldf.Date.max()\n",
        "\n",
        "  train_start_date = first_game_date + pd.Timedelta(days = train_start_buffer)\n",
        "\n",
        "  print('First and Last Game Dates in Fold: ' + str(first_game_date) + ' ' + str(last_game_date.date))\n",
        "  print('Start Date after Buffer: ' + str(train_start_date))\n",
        "  print('Validation Seasons: ' + str(alldf[alldf.Season >= validation_split].Season.unique()))\n",
        "\n",
        "  #Merge in other stats\n",
        "  if feature_list != None:\n",
        "    otherlist = [col for col in feature_list if col in otherstats.columns]\n",
        "    otherfeatures = otherstats[['Date', 'PlayerId'] + otherlist]\n",
        "  otherfeatures = otherstats\n",
        "\n",
        "  alldf = alldf.merge(otherfeatures, on = ['Date', 'PlayerId'], how = 'left')\n",
        "\n",
        "  print('Number of Player Games: ' + str(len(alldf)))\n",
        "\n",
        "  #Cut off at training start date\n",
        "  alldf = alldf[alldf.Date >= train_start_date]\n",
        "  print('Number of Player Games After Buffer: ' + str(len(alldf)))\n",
        "\n",
        "  #Combine all Input Features\n",
        "  features = pd.concat([gameavgstats[(gameavgstats.Date >= first_game_date) & (gameavgstats.Date <= last_game_date)],\n",
        "                        sumstats[(sumstats.Date >= first_game_date) & (sumstats.Date <= last_game_date)],\n",
        "                        teamstats[(teamstats.Date >= first_game_date) & (teamstats.Date <= last_game_date)]])\n",
        "\n",
        "  #Select features if provided list\n",
        "  if (feature_list != None):\n",
        "    features = features[features.StatName.isin(feature_list)]\n",
        "\n",
        "  #Number of Features for a Player Game\n",
        "  features['StatCount'] = features.groupby(['Date', 'PlayerId'])['StatName'].transform('size')\n",
        "  features['NonNACount'] = features.groupby(['Date', 'PlayerId'])['Data'].transform('count')\n",
        "  features['Coverage'] = features['NonNACount'] / features['StatCount']\n",
        "\n",
        "  #Remove rows with not enough feature coverage\n",
        "  features = features[features.Coverage > feature_cover_cutoff]\n",
        "\n",
        "  #Impute missing feature data first using last known value\n",
        "  features['Data_Imp'] = features['Data']\n",
        "  features = features.sort_values('Date')\n",
        "  features['Data_Imp'] = features.groupby(['PlayerId', 'StatName'])['Data_Imp'].ffill()\n",
        "\n",
        "  #Impute Missing FT Data with 0\n",
        "  features.loc[features.StatName.str.contains(\"FT\") & (features.Data_Imp.isna()), 'Data_Imp'] = 0\n",
        "\n",
        "  #Pivot Features into Wide\n",
        "  features = features.pivot(index = ['Date', 'PlayerId'], columns = 'StatName', values = 'Data_Imp').reset_index()\n",
        "\n",
        "  #Merge features back to player data\n",
        "  alldf = alldf.merge(features, on = ['Date', 'PlayerId'], how = 'left')\n",
        "\n",
        "  #Drop rows that still contains NAs\n",
        "  alldf = alldf.dropna()\n",
        "  print('Number of Player Games After Dropping NAs: ' + str(len(alldf)))\n",
        "\n",
        "  #Confirm no duplicated games\n",
        "  assert len(alldf[alldf.duplicated(subset = ['PlayerId', 'Date'], keep = False)]) == 0\n",
        "\n",
        "  #Split into train and validation sets\n",
        "  train_df, valid_df = alldf.loc[alldf.Season < validation_split], alldf.loc[alldf.Season >= validation_split]\n",
        "\n",
        "  #Testing set, both train and validation are the test set\n",
        "  if testset:\n",
        "    train_df, valid_df = alldf, alldf\n",
        "\n",
        "  ##Drop Not Needed Columns\n",
        "  train_df = train_df.drop(columns = ['PlayerId', 'Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS'])\n",
        "  valid_df = valid_df.drop(columns = ['PlayerId', 'Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS'])\n",
        "\n",
        "  #Split Binary and Numeric Columns\n",
        "  binary_cols = ['HomeGame', 'C',\t'C-F',\t'F',\t'F-C',\t'F-G',\t'G',\t'G-F']\n",
        "  numeric_cols = [col for col in train_df.columns if col not in binary_cols]\n",
        "\n",
        "  train_binary = train_df[binary_cols]\n",
        "  train_numeric = train_df[numeric_cols]\n",
        "  valid_binary = valid_df[binary_cols]\n",
        "  valid_numeric = valid_df[numeric_cols]\n",
        "\n",
        "  ##Standardize numerical features\n",
        "  scaler = StandardScaler().fit(train_numeric)\n",
        "  train_numeric = pd.DataFrame(scaler.transform(train_numeric), columns = numeric_cols)\n",
        "  valid_numeric = pd.DataFrame(scaler.transform(valid_numeric), columns = numeric_cols)\n",
        "\n",
        "  #Recombine binary and standardized numeric features\n",
        "  train_df = pd.concat([train_binary.reset_index(drop = True), train_numeric], axis = 1)\n",
        "  valid_df = pd.concat([valid_binary.reset_index(drop = True), valid_numeric], axis = 1)\n",
        "\n",
        "  feature_cols = train_df.columns\n",
        "\n",
        "  train_df = train_df.astype(float)\n",
        "  valid_df = valid_df.astype(float)\n",
        "\n",
        "  train_df = torch.tensor(train_df.to_numpy(), dtype = torch.float32)\n",
        "  valid_df = torch.tensor(valid_df.to_numpy(), dtype = torch.float32)\n",
        "\n",
        "  #Get target variable tensors\n",
        "  train_target, valid_target = alldf.loc[alldf.Season < validation_split, 'FTSYPTS'], alldf.loc[alldf.Season >= validation_split, 'FTSYPTS']\n",
        "\n",
        "  train_target = torch.tensor(train_target.to_numpy(), dtype = torch.float32)\n",
        "  valid_target = torch.tensor(valid_target.to_numpy(), dtype = torch.float32)\n",
        "\n",
        "  print('Train Shape', train_df.shape, 'Validation Shape', valid_df.shape, 'Train Target Shape', train_target.shape, 'Valid Target Shape', valid_target.shape)\n",
        "\n",
        "  return(train_df, valid_df, train_target, valid_target, feature_cols)"
      ],
      "metadata": {
        "id": "1Tl_LYcpgtnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Player, Team, and Opp Team indices for embeddings to data\n",
        "def get_data_embed_fold(fold_start, fold_end, validation_years = 1, train_start_buffer = 60,  feature_cover_cutoff = 0.6, feature_list = None, testset = False):\n",
        "\n",
        "  validation_split = fold_end - validation_years + 1\n",
        "\n",
        "  print(\"STARTING DATA PREP FOR FOLD\")\n",
        "  #Get data for seasons in fold\n",
        "  alldf = cleandata.loc[(cleandata.Season >= fold_start) & (cleandata.Season <= fold_end), ['PlayerId','Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS']]\n",
        "\n",
        "  first_game_date = alldf.Date.min()\n",
        "  last_game_date = alldf.Date.max()\n",
        "\n",
        "  train_start_date = first_game_date + pd.Timedelta(days = train_start_buffer)\n",
        "\n",
        "  print('First and Last Game Dates in Fold: ' + str(first_game_date) + ' ' + str(last_game_date.date))\n",
        "  print('Start Date after Buffer: ' + str(train_start_date))\n",
        "  print('Validation Seasons: ' + str(alldf[alldf.Season >= validation_split].Season.unique()))\n",
        "\n",
        "  #Merge in other stats\n",
        "  if feature_list != None:\n",
        "    otherlist = [col for col in feature_list if col in otherstats.columns]\n",
        "    otherfeatures = otherstats[['Date', 'PlayerId'] + otherlist]\n",
        "  otherfeatures = otherstats\n",
        "\n",
        "  alldf = alldf.merge(otherfeatures, on = ['Date', 'PlayerId'], how = 'left')\n",
        "\n",
        "  print('Number of Player Games: ' + str(len(alldf)))\n",
        "\n",
        "  #Cut off at training start date\n",
        "  alldf = alldf[alldf.Date >= train_start_date]\n",
        "  print('Number of Player Games After Buffer: ' + str(len(alldf)))\n",
        "\n",
        "  #Combine all Input Features\n",
        "  features = pd.concat([gameavgstats[(gameavgstats.Date >= first_game_date) & (gameavgstats.Date <= last_game_date)],\n",
        "                        sumstats[(sumstats.Date >= first_game_date) & (sumstats.Date <= last_game_date)],\n",
        "                        teamstats[(teamstats.Date >= first_game_date) & (teamstats.Date <= last_game_date)]])\n",
        "\n",
        "  #Select features if provided list\n",
        "  if (feature_list != None):\n",
        "    features = features[features.StatName.isin(feature_list)]\n",
        "\n",
        "  #Number of Features for a Player Game\n",
        "  features['StatCount'] = features.groupby(['Date', 'PlayerId'])['StatName'].transform('size')\n",
        "  features['NonNACount'] = features.groupby(['Date', 'PlayerId'])['Data'].transform('count')\n",
        "  features['Coverage'] = features['NonNACount'] / features['StatCount']\n",
        "\n",
        "  #Remove rows with not enough feature coverage\n",
        "  features = features[features.Coverage > feature_cover_cutoff]\n",
        "\n",
        "  #Impute missing feature data first using last known value\n",
        "  features['Data_Imp'] = features['Data']\n",
        "  features = features.sort_values('Date')\n",
        "  features['Data_Imp'] = features.groupby(['PlayerId', 'StatName'])['Data_Imp'].ffill()\n",
        "\n",
        "  #Impute Missing FT Data with 0\n",
        "  features.loc[features.StatName.str.contains(\"FT\") & (features.Data_Imp.isna()), 'Data_Imp'] = 0\n",
        "\n",
        "  #Pivot Features into Wide\n",
        "  features = features.pivot(index = ['Date', 'PlayerId'], columns = 'StatName', values = 'Data_Imp').reset_index()\n",
        "\n",
        "  #Merge features back to player data\n",
        "  alldf = alldf.merge(features, on = ['Date', 'PlayerId'], how = 'left')\n",
        "\n",
        "  #Drop rows that still contains NAs\n",
        "  alldf = alldf.dropna()\n",
        "  print('Number of Player Games After Dropping NAs: ' + str(len(alldf)))\n",
        "\n",
        "  #Confirm no duplicated games\n",
        "  assert len(alldf[alldf.duplicated(subset = ['PlayerId', 'Date'], keep = False)]) == 0\n",
        "\n",
        "  #Create Player, Team, and Opp Indices for embeddings\n",
        "  playerid_index = alldf.PlayerId.unique()\n",
        "  playerid_index = {playerid_index[i]: i for i in range(len(playerid_index))}\n",
        "  alldf['PlayerId_Index'] = alldf.PlayerId.map(playerid_index)\n",
        "\n",
        "  team_index = alldf.Playerteam.unique()\n",
        "  team_index = {team_index[i]: i for i in range(len(team_index))}\n",
        "  alldf['Team_Index'] = alldf.Playerteam.map(team_index)\n",
        "\n",
        "  team_index = alldf.Oppteam.unique()\n",
        "  team_index = {team_index[i]: i for i in range(len(team_index))}\n",
        "  alldf['Opp_Index'] = alldf.Oppteam.map(team_index)\n",
        "\n",
        "  # #Split into train and validation sets\n",
        "  train_df, valid_df = alldf.loc[alldf.Season < validation_split], alldf.loc[alldf.Season >= validation_split]\n",
        "\n",
        "  #Separate embedding columns\n",
        "  train_player_index = torch.tensor(train_df[['PlayerId_Index']].to_numpy(), dtype = torch.long)\n",
        "  train_team_index = torch.tensor(train_df[['Team_Index']].to_numpy(), dtype = torch.long)\n",
        "  train_opp_index = torch.tensor(train_df[['Opp_Index']].to_numpy(), dtype = torch.long)\n",
        "\n",
        "  valid_player_index = torch.tensor(valid_df[['PlayerId_Index']].to_numpy(), dtype = torch.long)\n",
        "  valid_team_index = torch.tensor(valid_df[['Team_Index']].to_numpy(), dtype = torch.long)\n",
        "  valid_opp_index = torch.tensor(valid_df[['Opp_Index']].to_numpy(), dtype = torch.long)\n",
        "\n",
        "  #Drop Not Needed Columns\n",
        "  train_df = train_df.drop(columns = ['PlayerId', 'Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS', 'PlayerId_Index', 'Team_Index', 'Opp_Index'])\n",
        "  valid_df = valid_df.drop(columns = ['PlayerId', 'Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS', 'PlayerId_Index', 'Team_Index', 'Opp_Index'])\n",
        "\n",
        "  #Split Binary and Numeric Columns\n",
        "  binary_cols = ['HomeGame', 'C',\t'C-F',\t'F',\t'F-C',\t'F-G',\t'G',\t'G-F']\n",
        "  numeric_cols = [col for col in train_df.columns if col not in binary_cols]\n",
        "\n",
        "  train_binary = train_df[binary_cols]\n",
        "  train_numeric = train_df[numeric_cols]\n",
        "  valid_binary = valid_df[binary_cols]\n",
        "  valid_numeric = valid_df[numeric_cols]\n",
        "\n",
        "  #Standardize numerical features\n",
        "  scaler = StandardScaler().fit(train_numeric)\n",
        "  train_numeric = pd.DataFrame(scaler.transform(train_numeric), columns = numeric_cols)\n",
        "  valid_numeric = pd.DataFrame(scaler.transform(valid_numeric), columns = numeric_cols)\n",
        "\n",
        "  #Recombine binary and standardized numeric features\n",
        "  train_df = pd.concat([train_binary.reset_index(drop = True), train_numeric], axis = 1)\n",
        "  valid_df = pd.concat([valid_binary.reset_index(drop = True), valid_numeric], axis = 1)\n",
        "\n",
        "  train_df = train_df.astype(float)\n",
        "  valid_df = valid_df.astype(float)\n",
        "\n",
        "  train_df = torch.tensor(train_df.to_numpy(), dtype = torch.float32)\n",
        "  valid_df = torch.tensor(valid_df.to_numpy(), dtype = torch.float32)\n",
        "\n",
        "  #Get target variable tensors\n",
        "  train_target, valid_target = alldf.loc[alldf.Season < validation_split, 'FTSYPTS'], alldf.loc[alldf.Season >= validation_split, 'FTSYPTS']\n",
        "\n",
        "  train_target = torch.tensor(train_target.to_numpy(), dtype = torch.float32)\n",
        "  valid_target = torch.tensor(valid_target.to_numpy(), dtype = torch.float32)\n",
        "\n",
        "  print('Train Shape', train_df.shape, 'Validation Shape', valid_df.shape, 'Train Target Shape', train_target.shape, 'Valid Target Shape', valid_target.shape)\n",
        "  print('Player Index Shape', train_player_index.shape, 'Team Index Shape', train_team_index.shape, 'Opp Index Shape', train_opp_index.shape)\n",
        "\n",
        "  return(train_df, valid_df, train_target, valid_target, train_player_index, valid_player_index, train_team_index, valid_team_index, train_opp_index, valid_opp_index)"
      ],
      "metadata": {
        "id": "0APo87mwaNR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepares data for LSTM models\n",
        "def get_data_fold_lstm(fold_start, fold_end, validation_years = 1, train_start_buffer = 60,  feature_cover_cutoff = 0.6, feature_list = None):\n",
        "\n",
        "  validation_split = fold_end - validation_years + 1\n",
        "\n",
        "  print(\"STARTING DATA PREP FOR FOLD\")\n",
        "  #Get data for seasons in fold\n",
        "  alldf = cleandata.loc[(cleandata.Season >= fold_start) & (cleandata.Season <= fold_end), ['PlayerId','Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS']]\n",
        "\n",
        "  first_game_date = alldf.Date.min()\n",
        "  last_game_date = alldf.Date.max()\n",
        "\n",
        "  train_start_date = first_game_date + pd.Timedelta(days = train_start_buffer)\n",
        "\n",
        "  print('First and Last Game Dates in Fold: ' + str(first_game_date) + ' ' + str(last_game_date.date))\n",
        "  print('Start Date after Buffer: ' + str(train_start_date))\n",
        "  print('Validation Seasons: ' + str(alldf[alldf.Season >= validation_split].Season.unique()))\n",
        "\n",
        "  #Merge in other stats\n",
        "  if feature_list != None:\n",
        "    otherlist = [col for col in feature_list if col in otherstats.columns]\n",
        "    otherfeatures = otherstats[['Date', 'PlayerId'] + otherlist]\n",
        "  otherfeatures = otherstats\n",
        "\n",
        "  alldf = alldf.merge(otherfeatures, on = ['Date', 'PlayerId'], how = 'left')\n",
        "\n",
        "  print('Number of Player Games: ' + str(len(alldf)))\n",
        "\n",
        "  #Cut off at training start date\n",
        "  alldf = alldf[alldf.Date >= train_start_date]\n",
        "  print('Number of Player Games After Buffer: ' + str(len(alldf)))\n",
        "\n",
        "  #Combine all Input Features\n",
        "  features = pd.concat([gameavgstats[(gameavgstats.Date >= first_game_date) & (gameavgstats.Date <= last_game_date)],\n",
        "                        sumstats[(sumstats.Date >= first_game_date) & (sumstats.Date <= last_game_date)],\n",
        "                        teamstats[(teamstats.Date >= first_game_date) & (teamstats.Date <= last_game_date)]])\n",
        "\n",
        "  #Select features if provided list\n",
        "  if (feature_list != None):\n",
        "    features = features[features.StatName.isin(feature_list)]\n",
        "\n",
        "  #Number of Features for a Player Game\n",
        "  features['StatCount'] = features.groupby(['Date', 'PlayerId'])['StatName'].transform('size')\n",
        "  features['NonNACount'] = features.groupby(['Date', 'PlayerId'])['Data'].transform('count')\n",
        "  features['Coverage'] = features['NonNACount'] / features['StatCount']\n",
        "\n",
        "  #Remove rows with not enough feature coverage\n",
        "  features = features[features.Coverage > feature_cover_cutoff]\n",
        "\n",
        "  #Impute missing feature data first using last known value\n",
        "  features['Data_Imp'] = features['Data']\n",
        "  features = features.sort_values('Date')\n",
        "  features['Data_Imp'] = features.groupby(['PlayerId', 'StatName'])['Data_Imp'].ffill()\n",
        "\n",
        "  #Impute Missing FT Data with 0\n",
        "  features.loc[features.StatName.str.contains(\"FT\") & (features.Data_Imp.isna()), 'Data_Imp'] = 0\n",
        "\n",
        "  #Pivot Features into Wide\n",
        "  features = features.pivot(index = ['Date', 'PlayerId'], columns = 'StatName', values = 'Data_Imp').reset_index()\n",
        "\n",
        "  #Merge features back to player data\n",
        "  alldf = alldf.merge(features, on = ['Date', 'PlayerId'], how = 'left')\n",
        "\n",
        "  #Drop rows that still contains NAs\n",
        "  alldf = alldf.dropna()\n",
        "  alldf = alldf.sort_values(['PlayerId','Date']).reset_index(drop=True)\n",
        "  print('Number of Player Games After Dropping NAs: ' + str(len(alldf)))\n",
        "\n",
        "  #Confirm no duplicated games\n",
        "  assert len(alldf[alldf.duplicated(subset = ['PlayerId', 'Date'], keep = False)]) == 0\n",
        "\n",
        "  #Split into train and validation sets\n",
        "  train_df, valid_df = alldf.loc[alldf.Season < validation_split], alldf.loc[alldf.Season >= validation_split]\n",
        "\n",
        "  ##Drop Not Needed Columns\n",
        "  train_df = train_df.drop(columns = ['PlayerId', 'Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS'])\n",
        "  valid_df = valid_df.drop(columns = ['PlayerId', 'Playerteam', 'Oppteam', 'Date', 'Season', 'FTSYPTS'])\n",
        "\n",
        "  #Split Binary and Numeric Columns\n",
        "  binary_cols = ['HomeGame', 'C',\t'C-F',\t'F',\t'F-C',\t'F-G',\t'G',\t'G-F']\n",
        "  numeric_cols = [col for col in train_df.columns if col not in binary_cols]\n",
        "\n",
        "  train_binary = train_df[binary_cols]\n",
        "  train_numeric = train_df[numeric_cols]\n",
        "  valid_binary = valid_df[binary_cols]\n",
        "  valid_numeric = valid_df[numeric_cols]\n",
        "\n",
        "  ##Standardize numerical features\n",
        "  scaler = StandardScaler().fit(train_numeric)\n",
        "  train_numeric = pd.DataFrame(scaler.transform(train_numeric), columns = numeric_cols)\n",
        "  valid_numeric = pd.DataFrame(scaler.transform(valid_numeric), columns = numeric_cols)\n",
        "\n",
        "  #Recombine binary and standardized numeric features\n",
        "  train_df = pd.concat([train_binary.reset_index(drop = True), train_numeric], axis = 1)\n",
        "  valid_df = pd.concat([valid_binary.reset_index(drop = True), valid_numeric], axis = 1)\n",
        "\n",
        "  train_df = train_df.astype(float)\n",
        "  valid_df = valid_df.astype(float)\n",
        "\n",
        "  display(train_df)\n",
        "  display(valid_df)\n",
        "\n",
        "  train_df = torch.tensor(train_df.to_numpy(), dtype = torch.float32)\n",
        "  valid_df = torch.tensor(valid_df.to_numpy(), dtype = torch.float32)\n",
        "\n",
        "  #Get target variable tensors\n",
        "  train_target, valid_target = alldf.loc[alldf.Season < validation_split, 'FTSYPTS'], alldf.loc[alldf.Season >= validation_split, 'FTSYPTS']\n",
        "\n",
        "  train_target = torch.tensor(train_target.to_numpy(), dtype = torch.float32)\n",
        "  valid_target = torch.tensor(valid_target.to_numpy(), dtype = torch.float32)\n",
        "\n",
        "  print('Train Shape', train_df.shape, 'Validation Shape', valid_df.shape, 'Train Target Shape', train_target.shape, 'Valid Target Shape', valid_target.shape)\n",
        "\n",
        "  return(train_df, valid_df, train_target, valid_target)"
      ],
      "metadata": {
        "id": "6U7UqfHrkvc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data Loaders"
      ],
      "metadata": {
        "id": "9ExTDhDc5FA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Creates list of data loaders with TensorDatasets wrapped around the training and validation data across folds\n",
        "def folds_data_loader(folds = [(1992, 2000), (1998, 2006), (2003, 2011), (2008, 2016)], valid_years = 2, train_buffer = 60,\n",
        "                      batch_size = 64, shuffle = True, feature_list = None, testset = True):\n",
        "\n",
        "  df_list = []\n",
        "\n",
        "  for i in folds:\n",
        "    print(\"Fold Start\", i[0], \"Fold End Season\", i[1])\n",
        "    train_df, valid_df, train_target, valid_target, feature_cols = get_data_fold(fold_start = i[0], fold_end = i[1], feature_list = feature_list,\n",
        "                                                                   validation_years = valid_years, train_start_buffer = train_buffer,\n",
        "                                                                   feature_cover_cutoff = 0.6)\n",
        "    train_dataset = TensorDataset(train_df, train_target)\n",
        "    valid_dataset = TensorDataset(valid_df, valid_target)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = shuffle, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    df_list.append((i[0], i[1], train_loader, valid_loader, feature_cols))\n",
        "\n",
        "  return(df_list)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qH4C0hA2caWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders_test = folds_data_loader(folds = [(2004, 2019)], valid_years=3, train_buffer = 0,\n",
        "#                                        batch_size = 64, shuffle = True, feature_list = None)"
      ],
      "metadata": {
        "id": "IdR_7-CDPFUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Creates list of data loaders with TensorDatasets wrapped around the training and validation data across folds, includes embeddings\n",
        "def folds_data_embed_loader(folds = [(1992, 2000), (1998, 2006), (2003, 2011), (2008, 2016)], batch_size = 64, shuffle = True, feature_list = None):\n",
        "\n",
        "  df_list = []\n",
        "\n",
        "  for i in folds:\n",
        "    print(\"Fold Start\", i[0], \"Fold End Season\", i[1])\n",
        "    train_df, valid_df, train_target, valid_target, train_player_index, valid_player_index, train_team_index, valid_team_index, train_opp_index, valid_opp_index = get_data_embed_fold(fold_start = i[0], fold_end = i[1], feature_list = feature_list,\n",
        "                                                                   validation_years = 2, train_start_buffer = 60,  feature_cover_cutoff = 0.6)\n",
        "\n",
        "    train_dataset = TensorDataset(train_df, train_target, train_player_index, train_team_index, train_opp_index )\n",
        "    valid_dataset = TensorDataset(valid_df, valid_target, valid_player_index, valid_team_index, valid_opp_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = shuffle, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    df_list.append((i[0], i[1], train_loader, valid_loader))\n",
        "\n",
        "  return(df_list)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "26OJ5ejNPUqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Create list of folds ###Creates list of data loaders with TensorDatasets wrapped around the training and validation data across folds, for the lstm model\n",
        "def folds_data_loader_lstm(folds = [(1992, 2000), (1998, 2006), (2003, 2011), (2008, 2016)], valid_years = 2, train_buffer = 60,\n",
        "                      batch_size = 64, shuffle = True, feature_list = None, testset = True):\n",
        "\n",
        "  df_list = []\n",
        "\n",
        "  for i in folds:\n",
        "    print(\"Fold Start\", i[0], \"Fold End Season\", i[1])\n",
        "    train_df, valid_df, train_target, valid_target = get_data_fold_lstm(fold_start = i[0], fold_end = i[1], feature_list = feature_list,\n",
        "                                                                   validation_years = valid_years, train_start_buffer = train_buffer,\n",
        "                                                                   feature_cover_cutoff = 0.6)\n",
        "    train_dataset = TensorDataset(train_df, train_target)\n",
        "    valid_dataset = TensorDataset(valid_df, valid_target)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = shuffle, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    df_list.append((i[0], i[1], train_loader, valid_loader))\n",
        "\n",
        "  return(df_list)\n",
        "\n",
        "dataloaders_lstm = folds_data_loader_lstm(folds = [(2004, 2019)], valid_years = 3, train_buffer = 0, batch_size = 32, shuffle = False, feature_list = None, testset = False)"
      ],
      "metadata": {
        "id": "i1cF--ZnlhEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the list of fold dfs.\n",
        "# with open('Data/full_fold_2008_test.pkl', 'wb') as f:\n",
        "#     pickle.dump(dataloaders_test, f)\n"
      ],
      "metadata": {
        "id": "Kg3z6X1eLGkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load list of fold dfs\n",
        "#full_fold_dfs_embeds\n",
        "#full_fold_dfs_shuffle_parallel\n",
        "#full_fold_2008_test.pkl\n",
        "\n",
        "# with open('Data/full_fold_2008_test.pkl', 'rb') as f:\n",
        "#     # dataloaders_embed = pkl.load(f)\n",
        "#     dataloaders = pkl.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "d3xlRXdPNkWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation"
      ],
      "metadata": {
        "id": "VwZMizrCCMPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard training for fully connected nn"
      ],
      "metadata": {
        "id": "oJ9dACoVlV1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####Training over the folds of data given the list of data loaders\n",
        "def train_model_folds(dataloaderlist, nepochs=3, lr=0.005, patience = 5):\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for dl in dataloaderlist:\n",
        "\n",
        "    #get train and validation data loaders\n",
        "    fold_start, fold_end = dl[0], dl[1]\n",
        "    train_loader, valid_loader = dl[2], dl[3]\n",
        "\n",
        "    #Get training sample sizes and feature dimension\n",
        "    n, d = train_loader.dataset.tensors[0].size()\n",
        "\n",
        "    #Get validation sample sizes and feature dimension\n",
        "    n_valid, d_valid = valid_loader.dataset.tensors[0].size()\n",
        "\n",
        "    #Batch Size\n",
        "    batch_size = train_loader.batch_size\n",
        "\n",
        "    print('NEW FOLD:', n, d, batch_size, fold_start, fold_end)\n",
        "\n",
        "    #Use GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    #Initiate the FCNN model\n",
        "    model = fc_nn(input_dim = d).to(device)\n",
        "\n",
        "    #Get the loss\n",
        "    loss_func = nn.L1Loss()\n",
        "\n",
        "    #Specify the optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "\n",
        "    #Early stopping setup\n",
        "    valid_loss_best = float('inf')\n",
        "    epochs_noimprovement = 0\n",
        "    early_stop = False\n",
        "\n",
        "    #Training loop over epochs\n",
        "    for epoch in range(nepochs):\n",
        "\n",
        "      print('Epoch', epoch)\n",
        "      model.train()\n",
        "      train_loss = 0\n",
        "\n",
        "      #Train over a batch\n",
        "      for x_batch, y_batch in train_loader:\n",
        "\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          #Forward pass of model\n",
        "          optimizer.zero_grad()\n",
        "          output = model(x_batch).squeeze(dim = 1)\n",
        "\n",
        "          loss = loss_func(output, y_batch)\n",
        "\n",
        "          #backward pass of model, backprop\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #Accumulate training loss\n",
        "          train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      #Compute average training loss per sample\n",
        "      train_loss /= n\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      #Run Validation\n",
        "      model.eval()\n",
        "      valid_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for x_batch, y_batch in valid_loader:\n",
        "\n",
        "              x_batch = x_batch.to(device)\n",
        "              y_batch = y_batch.to(device)\n",
        "\n",
        "              #Run forward pass of model\n",
        "              output = model(x_batch).squeeze(dim = 1)\n",
        "\n",
        "              #Compute Loss and sum\n",
        "              loss = loss_func(output, y_batch)\n",
        "\n",
        "              valid_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      #Compute average loss per sample\n",
        "      valid_loss /= n_valid\n",
        "      valid_losses.append(valid_loss)\n",
        "\n",
        "      #Early Stopping, if validation loss does not improve in #patience of epochs, stop training.\n",
        "      if valid_loss < valid_loss_best - 1e-5:\n",
        "        valid_loss_best = valid_loss\n",
        "        epochs_noimprovement = 0\n",
        "      else:\n",
        "        epochs_noimprovement += 1\n",
        "\n",
        "      if epochs_noimprovement >= patience:\n",
        "        print('Early Stopping due to No Improvement')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "      #Display and record losses\n",
        "      print(f\"Epoch {epoch+1:>2}/{nepochs}: Train Loss = {train_loss:.4f}, Val Loss = {valid_loss:.4f}, Best Val Loss = {valid_loss_best:.4f}\")\n",
        "\n",
        "      #Return the loss curve results\n",
        "      loss_curve = pd.DataFrame({'Train':train_losses, 'Valid':valid_losses}).reset_index(names = 'Epoch')\n",
        "\n",
        "      loss_curve['Fold'] = fold_start\n",
        "\n",
        "      results.append(loss_curve)\n",
        "\n",
        "  results = pd.concat(results)\n",
        "\n",
        "  return((results))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bjxFh9OEaP2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Embeddings"
      ],
      "metadata": {
        "id": "f2rPtWbihTvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####Training over the folds of data given the list of data loaders, including embeddings\n",
        "def train_model_embed_folds(dataloaderlist, nepochs=3, lr=0.005, patience = 5):\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for dl in dataloaderlist:\n",
        "\n",
        "    #get train and validation data loaders\n",
        "    fold_start, fold_end = dl[0], dl[1]\n",
        "    train_loader, valid_loader = dl[2], dl[3]\n",
        "\n",
        "    #Get training sample sizes and feature dimension\n",
        "    n, d = train_loader.dataset.tensors[0].size()\n",
        "\n",
        "    #Get validation sample sizes and feature dimension\n",
        "    n_valid, d_valid = valid_loader.dataset.tensors[0].size()\n",
        "\n",
        "    #Batch Size\n",
        "    batch_size = train_loader.batch_size\n",
        "\n",
        "    #Get Embedding dimensions for players, teams and opponents\n",
        "    numplayers = max(max(train_loader.dataset.tensors[2].unique()), max(valid_loader.dataset.tensors[2].unique())) + 1\n",
        "    numteams = max(max(train_loader.dataset.tensors[3].unique()), max(valid_loader.dataset.tensors[3].unique())) + 1\n",
        "    numopps = max(max(train_loader.dataset.tensors[4].unique()), max(valid_loader.dataset.tensors[4].unique())) + 1\n",
        "\n",
        "    print('NEW FOLD:', 'Num Samples', n, 'Input Dim', d, 'Batch Size', batch_size, 'Fold Seasons', fold_start, fold_end)\n",
        "    print('Players', numplayers, 'Teams', numteams, 'Opps', numopps)\n",
        "\n",
        "    #Use GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "    #Initiate a FCNN model with embeddings\n",
        "    model = fc_nn_embed(input_dim = d, num_players = numplayers, num_teams = numteams, num_opps = numopps).to(device)\n",
        "\n",
        "    #Use L1 Loss\n",
        "    loss_func = nn.L1Loss()\n",
        "\n",
        "    #Specify the optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "\n",
        "    #Early stopping setup\n",
        "    valid_loss_best = float('inf')\n",
        "    epochs_noimprovement = 0\n",
        "    early_stop = False\n",
        "\n",
        "    #Training loop over epochs\n",
        "    for epoch in range(nepochs):\n",
        "\n",
        "      print('Epoch', epoch)\n",
        "      model.train()\n",
        "      train_loss = 0\n",
        "\n",
        "      #Take a batch of features, targets, and ids for embeddings\n",
        "      for x_batch, y_batch, playerids, teamids, oppids in train_loader:\n",
        "\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.to(device)\n",
        "          playerids = playerids.to(device)\n",
        "          teamids = teamids.to(device)\n",
        "          oppids = oppids.to(device)\n",
        "\n",
        "          #Forward pass of model\n",
        "          optimizer.zero_grad()\n",
        "          output = model(x_batch, player_ids = playerids, team_ids = teamids, opp_ids = oppids).squeeze(dim = 1)\n",
        "\n",
        "          loss = loss_func(output, y_batch)\n",
        "\n",
        "          #backward pass of model\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #Accumulate training loss\n",
        "          train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      #Compute average training loss per sample\n",
        "      train_loss /= n\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      #Run Validation\n",
        "      model.eval()\n",
        "      valid_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for x_batch, y_batch, playerids, teamids, oppids in valid_loader:\n",
        "\n",
        "              x_batch = x_batch.to(device)\n",
        "              y_batch = y_batch.to(device)\n",
        "              playerids = playerids.to(device)\n",
        "              teamids = teamids.to(device)\n",
        "              oppids = oppids.to(device)\n",
        "\n",
        "              #Run forward pass of model\n",
        "              output = model(x_batch, player_ids = playerids, team_ids = teamids, opp_ids = oppids).squeeze(dim = 1)\n",
        "\n",
        "              #Compute Loss and sum\n",
        "              loss = loss_func(output, y_batch)\n",
        "\n",
        "              valid_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      #Compute average loss per sample\n",
        "      valid_loss /= n_valid\n",
        "      valid_losses.append(valid_loss)\n",
        "\n",
        "      #Early Stopping\n",
        "      if valid_loss < valid_loss_best - 1e-5:\n",
        "        valid_loss_best = valid_loss\n",
        "        epochs_noimprovement = 0\n",
        "      else:\n",
        "        epochs_noimprovement += 1\n",
        "\n",
        "      if epochs_noimprovement >= patience:\n",
        "        print('Early Stopping due to No Improvement')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "      #Display and record losses\n",
        "      print(f\"Epoch {epoch+1:>2}/{nepochs}: Train Loss = {train_loss:.4f}, Val Loss = {valid_loss:.4f}, Best Val Loss = {valid_loss_best:.4f}\")\n",
        "\n",
        "      loss_curve = pd.DataFrame({'Train':train_losses, 'Valid':valid_losses}).reset_index(names = 'Epoch')\n",
        "\n",
        "      loss_curve['Fold'] = fold_start\n",
        "\n",
        "      results.append(loss_curve)\n",
        "\n",
        "  results = pd.concat(results)\n",
        "\n",
        "  return(results)\n"
      ],
      "metadata": {
        "id": "tBJoyn1Gg7XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Training"
      ],
      "metadata": {
        "id": "r81ySMUFmQn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####Training over folds of the dataset for the LSTM\n",
        "def train_model_lstm_folds(dataloaderlist, nepochs=3, lr=0.005, patience = 5):\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for dl in dataloaderlist:\n",
        "\n",
        "    #get train and validation data loaders\n",
        "    fold_start, fold_end = dl[0], dl[1]\n",
        "    train_loader, valid_loader = dl[2], dl[3]\n",
        "\n",
        "    #Get training sample sizes and feature dimension\n",
        "    n, d = train_loader.dataset.tensors[0].size()\n",
        "\n",
        "    #Get validation sample sizes and feature dimension\n",
        "    n_valid, d_valid = valid_loader.dataset.tensors[0].size()\n",
        "\n",
        "    #Batch Size\n",
        "    batch_size = train_loader.batch_size\n",
        "\n",
        "    print('NEW FOLD:', n, d, batch_size, fold_start, fold_end)\n",
        "\n",
        "    #Use GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    #Get the model\n",
        "    model = lstm_nn(input_dim = d).to(device)\n",
        "\n",
        "    #Get the loss\n",
        "    loss_func = nn.L1Loss()\n",
        "\n",
        "    #Specify the optimizer\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay = 1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                           mode='min',\n",
        "                                                           factor=0.5,\n",
        "                                                           patience=3)\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "\n",
        "    #Early stopping setup\n",
        "    valid_loss_best = float('inf')\n",
        "    epochs_noimprovement = 0\n",
        "    early_stop = False\n",
        "\n",
        "    #Training loop over epochs\n",
        "    for epoch in range(nepochs):\n",
        "\n",
        "      print('Epoch', epoch)\n",
        "      model.train()\n",
        "      train_loss = 0.0\n",
        "\n",
        "      for x_batch, y_batch in train_loader:\n",
        "\n",
        "          x_batch = x_batch.to(device).unsqueeze(1)\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          #Forward pass of model\n",
        "          optimizer.zero_grad()\n",
        "          output = model(x_batch)#.squeeze(dim = 1)\n",
        "\n",
        "          loss = loss_func(output, y_batch)\n",
        "\n",
        "          #backward pass of model\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #Accumulate training loss\n",
        "          train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      #Compute average training loss per sample\n",
        "      train_loss /= n\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      #Run Validation\n",
        "      model.eval()\n",
        "      valid_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for x_batch, y_batch in valid_loader:\n",
        "\n",
        "              x_batch = x_batch.to(device).unsqueeze(1)\n",
        "              y_batch = y_batch.to(device)\n",
        "\n",
        "              #Run forward pass of model\n",
        "              output = model(x_batch)#.squeeze(dim = 1)\n",
        "\n",
        "              #Compute Loss and sum\n",
        "              loss = loss_func(output, y_batch)\n",
        "\n",
        "              valid_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      #Compute average loss per sample\n",
        "      valid_loss /= n_valid\n",
        "      valid_losses.append(valid_loss)\n",
        "\n",
        "      scheduler.step(valid_loss)\n",
        "\n",
        "      #Early Stopping\n",
        "      if valid_loss < valid_loss_best - 1e-5:\n",
        "        valid_loss_best = valid_loss\n",
        "        epochs_noimprovement = 0\n",
        "      else:\n",
        "        epochs_noimprovement += 1\n",
        "\n",
        "      if epochs_noimprovement >= patience:\n",
        "        print('Early Stopping due to No Improvement')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "      #Display and record losses\n",
        "      print(f\"Epoch {epoch+1:>2}/{nepochs}: Train Loss = {train_loss:.4f}, Val Loss = {valid_loss:.4f}, Best Val Loss = {valid_loss_best:.4f}\")\n",
        "\n",
        "      loss_curve = pd.DataFrame({'Train':train_losses, 'Valid':valid_losses}).reset_index(names = 'Epoch')\n",
        "\n",
        "      loss_curve['Fold'] = fold_start\n",
        "\n",
        "      results.append(loss_curve)\n",
        "\n",
        "  results = pd.concat(results)\n",
        "\n",
        "  return(results)\n"
      ],
      "metadata": {
        "id": "7LLOvd_kmSp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Baseline Models"
      ],
      "metadata": {
        "id": "4eR77XHuitQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the dataloaders to build Lasso linear regression models over folds for baseline comparison\n",
        "\n",
        "dataloaders_tmp = dataloaders_test\n",
        "for i in range(len(dataloaders_tmp)):\n",
        "\n",
        "  #Get train and validation data loaders\n",
        "  fold_start, fold_end = dataloaders_tmp[i][0], dataloaders_tmp[i][1]\n",
        "  train_loader, valid_loader = dataloaders_tmp[i][2], dataloaders_tmp[i][3]\n",
        "\n",
        "  #Get training sample sizes and feature dimension\n",
        "  n, d = train_loader.dataset.tensors[0].size()\n",
        "\n",
        "  print(\"Fold:\", fold_start, fold_end)\n",
        "\n",
        "  ##Extract training and validation features and targets\n",
        "  X_train = train_loader.dataset.tensors[0].cpu().numpy()\n",
        "  y_train = train_loader.dataset.tensors[1].cpu().numpy().flatten()\n",
        "\n",
        "  X_val = valid_loader.dataset.tensors[0].cpu().numpy()\n",
        "  y_val = valid_loader.dataset.tensors[1].cpu().numpy().flatten()\n",
        "\n",
        "  print(X_train.shape)\n",
        "\n",
        "  # Fit the linear regression model\n",
        "  lr_model = Lasso(alpha=0.1)\n",
        "  lr_model.fit(X_train, y_train)\n",
        "\n",
        "  # Show how many non zero coefficients there are\n",
        "  non_zero_count = np.sum(lr_model.coef_ != 0)\n",
        "  print(non_zero_count)\n",
        "\n",
        "  # Predict on validation set\n",
        "  y_pred = lr_model.predict(X_val)\n",
        "  y_train_pred = lr_model.predict(X_train)\n",
        "\n",
        "  #Compute MAE\n",
        "  train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "  valid_mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "  print('Train:', train_mae, 'Valid:', valid_mae)\n",
        "\n",
        "  #Show coefficients\n",
        "  coef_df = pd.DataFrame({\n",
        "      'Feature': dataloaders_tmp[0][4],\n",
        "      'Weight': lr_model.coef_})\n",
        "\n"
      ],
      "metadata": {
        "id": "d93WGX2IGnZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Neural Nets"
      ],
      "metadata": {
        "id": "2CSHxKiRwB0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fully Connected NN Models\n",
        "class fc_nn(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(fc_nn, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(256, 128),\n",
        "            # nn.ReLU(),\n",
        "            # # nn.Dropout(0.3),\n",
        "            # nn.Linear(128, 128),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "tgI_tx2iv_di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FCNN with Embedding"
      ],
      "metadata": {
        "id": "k5SKJ1Z_nmNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fully connected model with player and team embeddings\n",
        "class fc_nn_embed(nn.Module):\n",
        "    def __init__(self, input_dim, num_players, num_teams, num_opps):\n",
        "        super(fc_nn_embed, self).__init__()\n",
        "\n",
        "        #Specify embedding layers with hidden dimension\n",
        "        self.player_embed = nn.Embedding(num_players, 16)\n",
        "        self.team_embed = nn.Embedding(num_teams, 4)\n",
        "        self.opp_embed = nn.Embedding(num_opps, 4)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + 24, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.2),\n",
        "            # nn.Linear(64, 32),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, player_ids, team_ids, opp_ids):\n",
        "\n",
        "        #Generate embeddings\n",
        "        player_embeds = self.player_embed(player_ids.squeeze(-1))\n",
        "        team_embeds = self.team_embed(team_ids.squeeze(-1))\n",
        "        opp_embeds = self.opp_embed(opp_ids.squeeze(-1))\n",
        "\n",
        "        #Concatenate embeddings with numerical features\n",
        "        x = torch.cat([x, player_embeds, team_embeds, opp_embeds], dim=1)\n",
        "\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "oLGmkjNpiD-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "PzMpjQBMoT7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Model\n",
        "class lstm_nn(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers>1 else 0.0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        last_h = hn[-1]\n",
        "        last_h = self.dropout(last_h)\n",
        "        y = self.fc(last_h)\n",
        "        return y.squeeze(-1)"
      ],
      "metadata": {
        "id": "ePO-7Fwjl22_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Testing"
      ],
      "metadata": {
        "id": "jc-mn__GmzP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Run the training-validation functions\n",
        "results = train_model_folds(dataloaders_test, nepochs=2, lr=0.005, patience = 5)\n",
        "# results = train_model_embed_folds(dataloaders_embed, nepochs=30, lr=0.005, patience = 5)\n",
        "# results = train_model_lstm_folds(dataloaders_lstm, nepochs=15, lr=0.01, patience = 10)\n",
        "results = results.melt(id_vars = ['Fold', 'Epoch'], var_name = 'Type', value_name = 'Loss')\n"
      ],
      "metadata": {
        "id": "VWe_XjeBih4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results.to_csv('Data/results_base_fcnn_earlystop.csv')\n",
        "# results = pd.read_csv('Data/results_base_fcnn_dropout.csv')"
      ],
      "metadata": {
        "id": "T_g3D8OUlYDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show best loss results and number of epochs\n",
        "display(results.groupby(['Fold', 'Type'])['Loss'].min().reset_index().pivot(index = 'Type', columns = 'Fold', values = 'Loss'))\n",
        "display(results.groupby(['Fold', 'Type'])['Epoch'].max().reset_index().pivot(index = 'Type', columns = 'Fold', values = 'Epoch'))\n",
        "\n",
        "#Plot training and validation loss curves\n",
        "sns.set_context(rc={\"font.size\": 14, \"axes.titlesize\": 18,https://www.gradescope.com/courses/994586 \"axes.labelsize\": 16})\n",
        "x = sns.relplot(data = results, x = 'Epoch', y = 'Loss', hue = 'Type', col = 'Fold', kind = 'line', col_wrap=4)\n",
        "x.fig.suptitle('Loss Curves for Fully Connected Network with 3 layers with Early Stopping', fontsize = 16)\n",
        "x.fig.subplots_adjust(top=0.8)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TaLBfpTbgOQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}